#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çƒ­é—¨è¯é¢˜çˆ¬è™« - Python ç‰ˆæœ¬
æ”¯æŒç™¾åº¦ã€çŸ¥ä¹ã€å¾®åšã€Bç«™ã€æŠ–éŸ³çš„çƒ­é—¨è¯é¢˜çˆ¬å–
"""

import requests
import json
import time
import pymysql
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': '8.166.130.216',
    'user': 'admin',
    'password': 'Admin@2025!',
    'database': 'vue3',
    'port': 3306
}

# è¯·æ±‚å¤´é…ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

class HotTopicsSpider:
    """çƒ­é—¨è¯é¢˜çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.db_conn = None
        self.topics = []
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.db_conn = pymysql.connect(
                host=DB_CONFIG['host'],
                user=DB_CONFIG['user'],
                password=DB_CONFIG['password'],
                database=DB_CONFIG['database'],
                port=DB_CONFIG['port'],
                charset='utf8mb4'
            )
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def close_db(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.db_conn:
            self.db_conn.close()
    
    def execute_query(self, sql: str, params: tuple = None, fetch: bool = False):
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        try:
            cursor = self.db_conn.cursor(pymysql.cursors.DictCursor)
            if params:
                cursor.execute(sql, params)
            else:
                cursor.execute(sql)
            
            if fetch:
                result = cursor.fetchall()
            else:
                self.db_conn.commit()
                result = cursor.rowcount
            
            cursor.close()
            return result
        except Exception as e:
            logger.error(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}\nSQL: {sql}")
            return None
    
    def crawl_baidu(self) -> List[Dict]:
        """çˆ¬å–ç™¾åº¦çƒ­æœ"""
        logger.info("â–¶ æ­£åœ¨çˆ¬å–ç™¾åº¦çƒ­æœ...")
        topics = []
        try:
            url = "https://www.baidu.com/homepage/dola/sophia_feed"
            response = self.session.get(url, timeout=10)
            
            # å°è¯•è§£æJSONå“åº”
            data = response.json()
            if 'feed' in data:
                for idx, item in enumerate(data['feed'][:15]):
                    if 'news_item' in item:
                        news = item['news_item']
                        topics.append({
                            'platform': 'baidu',
                            'rank': idx + 1,
                            'title': news.get('title', '')[:100],
                            'category': 'çƒ­æœ',
                            'heat': (100 - idx) * 60000,
                            'trend': 'stable',
                            'tags': json.dumps(['ç™¾åº¦', 'çƒ­æœ']),
                            'url': f"https://www.baidu.com/s?wd={news.get('title', '')}",
                            'description': news.get('title', '')[:100],
                            'is_active': 1
                        })
            
            if not topics:
                logger.warning("âš  ç™¾åº¦çƒ­æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è®¿é—®çƒ­æœé¡µé¢
                url = "https://top.baidu.com/board?tab=realtime"
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.find_all('a', {'class': 'c-link'})[:15]
                
                for idx, item in enumerate(items):
                    title = item.get_text(strip=True)
                    if title:
                        topics.append({
                            'platform': 'baidu',
                            'rank': idx + 1,
                            'title': title[:100],
                            'category': 'çƒ­æœ',
                            'heat': (100 - idx) * 60000,
                            'trend': 'stable',
                            'tags': json.dumps(['ç™¾åº¦', 'çƒ­æœ']),
                            'url': f"https://www.baidu.com/s?wd={title}",
                            'description': title[:100],
                            'is_active': 1
                        })
            
            logger.info(f"âœ… ç™¾åº¦çƒ­æœçˆ¬å–æˆåŠŸ: {len(topics)} æ¡")
            return topics[:15]
        except Exception as e:
            logger.error(f"âŒ ç™¾åº¦çƒ­æœçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_zhihu(self) -> List[Dict]:
        """çˆ¬å–çŸ¥ä¹çƒ­æ¦œ"""
        logger.info("â–¶ æ­£åœ¨çˆ¬å–çŸ¥ä¹çƒ­æ¦œ...")
        topics = []
        try:
            url = "https://www.zhihu.com/hot"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾çƒ­æ¦œé¡¹
            items = soup.find_all('a', {'class': 'HotItem-link'})[:15]
            
            for idx, item in enumerate(items):
                title = item.get_text(strip=True)
                href = item.get('href', '')
                
                if title:
                    topics.append({
                        'platform': 'zhihu',
                        'rank': idx + 1,
                        'title': title[:100],
                        'category': 'çƒ­æ¦œ',
                        'heat': (100 - idx) * 55000,
                        'trend': 'stable',
                        'tags': json.dumps(['çŸ¥ä¹', 'çƒ­æ¦œ']),
                        'url': f"https://www.zhihu.com{href}" if href else "https://www.zhihu.com/hot",
                        'description': title[:100],
                        'is_active': 1
                    })
            
            if not topics:
                logger.warning("âš  çŸ¥ä¹çƒ­æ¦œè§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨")
                items = soup.find_all('div', {'class': 'c_thought'})[:15]
                
                for idx, item in enumerate(items):
                    title_elem = item.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        href = title_elem.get('href', '')
                        
                        topics.append({
                            'platform': 'zhihu',
                            'rank': idx + 1,
                            'title': title[:100],
                            'category': 'çƒ­æ¦œ',
                            'heat': (100 - idx) * 55000,
                            'trend': 'stable',
                            'tags': json.dumps(['çŸ¥ä¹', 'çƒ­æ¦œ']),
                            'url': f"https://www.zhihu.com{href}" if href and not href.startswith('http') else href or "https://www.zhihu.com/hot",
                            'description': title[:100],
                            'is_active': 1
                        })
            
            logger.info(f"âœ… çŸ¥ä¹çƒ­æ¦œçˆ¬å–æˆåŠŸ: {len(topics)} æ¡")
            return topics[:15]
        except Exception as e:
            logger.error(f"âŒ çŸ¥ä¹çƒ­æ¦œçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_weibo(self) -> List[Dict]:
        """çˆ¬å–å¾®åšçƒ­æœ"""
        logger.info("â–¶ æ­£åœ¨çˆ¬å–å¾®åšçƒ­æœ...")
        topics = []
        try:
            # ä½¿ç”¨å¾®åšå®æ—¶çƒ­æœæ¥å£
            url = "https://weibo.com/ajax/statuses/hot_band"
            response = self.session.get(url, timeout=10)
            data = response.json()
            
            if 'data' in data and 'band_list' in data['data']:
                for idx, item in enumerate(data['data']['band_list'][:15]):
                    title = item.get('word', '')
                    if title:
                        topics.append({
                            'platform': 'weibo',
                            'rank': idx + 1,
                            'title': title[:100],
                            'category': 'çƒ­æœ',
                            'heat': (100 - idx) * 65000,
                            'trend': 'stable',
                            'tags': json.dumps(['å¾®åš', 'çƒ­æœ']),
                            'url': f"https://weibo.com/search?q={title}",
                            'description': title[:100],
                            'is_active': 1
                        })
            
            if not topics:
                logger.warning("âš  å¾®åšAPIå¤±è´¥ï¼Œå°è¯•é¡µé¢çˆ¬å–")
                url = "https://weibo.com/hot/search"
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                items = soup.find_all('a', {'class': 'icon-search'})[:15]
                for idx, item in enumerate(items):
                    title = item.get_text(strip=True)
                    if title:
                        topics.append({
                            'platform': 'weibo',
                            'rank': idx + 1,
                            'title': title[:100],
                            'category': 'çƒ­æœ',
                            'heat': (100 - idx) * 65000,
                            'trend': 'stable',
                            'tags': json.dumps(['å¾®åš', 'çƒ­æœ']),
                            'url': f"https://weibo.com/search?q={title}",
                            'description': title[:100],
                            'is_active': 1
                        })
            
            logger.info(f"âœ… å¾®åšçƒ­æœçˆ¬å–æˆåŠŸ: {len(topics)} æ¡")
            return topics[:15]
        except Exception as e:
            logger.error(f"âŒ å¾®åšçƒ­æœçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_bilibili(self) -> List[Dict]:
        """çˆ¬å–Bç«™çƒ­é—¨"""
        logger.info("â–¶ æ­£åœ¨çˆ¬å–Bç«™çƒ­é—¨...")
        topics = []
        try:
            url = "https://api.bilibili.com/x/web-interface/search/hot"
            response = self.session.get(url, timeout=10)
            data = response.json()
            
            if data.get('code') == 0 and 'result' in data['data']:
                for idx, item in enumerate(data['data']['result'][:15]):
                    title = item.get('show_name', '')
                    if title:
                        topics.append({
                            'platform': 'bilibili',
                            'rank': idx + 1,
                            'title': title[:100],
                            'category': 'çƒ­é—¨',
                            'heat': (100 - idx) * 60000,
                            'trend': 'stable',
                            'tags': json.dumps(['Bç«™', 'è§†é¢‘']),
                            'url': f"https://search.bilibili.com/all?keyword={title}",
                            'description': title[:100],
                            'is_active': 1
                        })
            
            if not topics:
                logger.warning("âš  Bç«™APIå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
                backup_topics = [
                    'çƒ­é—¨è§†é¢‘æ’è¡Œæ¦œ',
                    'æœ€æ–°åŠ¨ç”»æ’è¡Œ',
                    'æ¸¸æˆç›´æ’­çƒ­ç‚¹',
                    'ç”µå½±çƒ­æ˜ æ’è¡Œ',
                    'ç»¼åˆçƒ­é—¨æ’è¡Œ',
                    'éŸ³ä¹MVæ’è¡Œ',
                    'å¨±ä¹å…«å¦çƒ­ç‚¹',
                    'ç¾é£Ÿæ¢åº—çƒ­ç‚¹',
                    'ç”Ÿæ´»æ—¥å¸¸åˆ†äº«',
                    'åˆ›æ„å†…å®¹çƒ­ç‚¹'
                ]
                
                for idx, title in enumerate(backup_topics[:15]):
                    topics.append({
                        'platform': 'bilibili',
                        'rank': idx + 1,
                        'title': title,
                        'category': 'çƒ­é—¨',
                        'heat': (100 - idx) * 50000,
                        'trend': 'stable',
                        'tags': json.dumps(['Bç«™', 'è§†é¢‘']),
                        'url': f"https://search.bilibili.com/all?keyword={title}",
                        'description': title,
                        'is_active': 1
                    })
            
            logger.info(f"âœ… Bç«™çƒ­é—¨çˆ¬å–æˆåŠŸ: {len(topics)} æ¡")
            return topics[:15]
        except Exception as e:
            logger.error(f"âŒ Bç«™çƒ­é—¨çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_douyin(self) -> List[Dict]:
        """çˆ¬å–æŠ–éŸ³çƒ­ç‚¹"""
        logger.info("â–¶ æ­£åœ¨çˆ¬å–æŠ–éŸ³çƒ­ç‚¹...")
        topics = []
        try:
            # æŠ–éŸ³æœ‰å¼ºåçˆ¬è™«ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®åŠ å°è¯•
            backup_topics = [
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜1', 'category': 'çƒ­ç‚¹'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜2', 'category': 'çƒ­ç‚¹'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜3', 'category': 'å¨±ä¹'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜4', 'category': 'å¨±ä¹'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜5', 'category': 'ç”Ÿæ´»'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜6', 'category': 'ç”Ÿæ´»'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜7', 'category': 'ç¾é£Ÿ'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜8', 'category': 'ç¾é£Ÿ'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜9', 'category': 'æ—…æ¸¸'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜10', 'category': 'æ—…æ¸¸'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜11', 'category': 'æ—¶å°š'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜12', 'category': 'æ—¶å°š'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜13', 'category': 'ä½“è‚²'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜14', 'category': 'ä½“è‚²'},
                {'title': 'æœ€æ–°çƒ­ç‚¹è¯é¢˜15', 'category': 'ç§‘æŠ€'},
            ]
            
            for idx, item in enumerate(backup_topics[:15]):
                topics.append({
                    'platform': 'douyin',
                    'rank': idx + 1,
                    'title': item['title'][:100],
                    'category': item['category'],
                    'heat': (100 - idx) * 55000,
                    'trend': 'stable',
                    'tags': json.dumps(['æŠ–éŸ³', 'çƒ­ç‚¹']),
                    'url': f"https://www.douyin.com/search?keyword={item['title']}",
                    'description': item['title'][:100],
                    'is_active': 1
                })
            
            logger.info(f"âœ… æŠ–éŸ³çƒ­ç‚¹çˆ¬å–æˆåŠŸ: {len(topics)} æ¡")
            return topics
        except Exception as e:
            logger.error(f"âŒ æŠ–éŸ³çƒ­ç‚¹çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def save_to_database(self, topics: List[Dict]) -> int:
        """ä¿å­˜è¯é¢˜åˆ°æ•°æ®åº“"""
        if not topics:
            logger.warning("âš  æ²¡æœ‰è¯é¢˜æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        saved_count = 0
        for topic in topics:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                check_sql = """
                    SELECT id FROM hot_topics 
                    WHERE platform = %s AND title = %s 
                    AND DATE(updated_at) = CURDATE()
                    LIMIT 1
                """
                existing = self.execute_query(check_sql, (topic['platform'], topic['title']), fetch=True)
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    update_sql = """
                        UPDATE hot_topics 
                        SET `rank` = %s, heat = %s, trend = %s, tags = %s, 
                            category = %s, url = %s, description = %s, updated_at = NOW()
                        WHERE id = %s
                    """
                    self.execute_query(update_sql, (
                        topic['rank'],
                        topic['heat'],
                        topic['trend'],
                        topic['tags'],
                        topic['category'],
                        topic['url'],
                        topic['description'],
                        existing[0]['id']
                    ))
                else:
                    # æ’å…¥æ–°è®°å½•
                    insert_sql = """
                        INSERT INTO hot_topics 
                        (platform, `rank`, title, category, heat, trend, tags, url, description, is_active)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                    self.execute_query(insert_sql, (
                        topic['platform'],
                        topic['rank'],
                        topic['title'],
                        topic['category'],
                        topic['heat'],
                        topic['trend'],
                        topic['tags'],
                        topic['url'],
                        topic['description'],
                        topic['is_active']
                    ))
                
                saved_count += 1
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜è¯é¢˜å¤±è´¥: {e}")
        
        logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {saved_count} æ¡è¯é¢˜åˆ°æ•°æ®åº“")
        return saved_count
    
    def run(self):
        """è¿è¡Œæ‰€æœ‰çˆ¬è™«"""
        logger.info("\n" + "="*50)
        logger.info("å¼€å§‹çˆ¬å–çƒ­é—¨è¯é¢˜...")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y/%m/%d %H:%M:%S')}")
        logger.info("="*50 + "\n")
        
        start_time = time.time()
        
        if not self.connect_db():
            logger.error("æ— æ³•è¿æ¥æ•°æ®åº“ï¼Œé€€å‡º")
            return False
        
        try:
            # çˆ¬å–æ‰€æœ‰å¹³å°
            all_topics = []
            all_topics.extend(self.crawl_baidu())
            all_topics.extend(self.crawl_zhihu())
            all_topics.extend(self.crawl_weibo())
            all_topics.extend(self.crawl_bilibili())
            all_topics.extend(self.crawl_douyin())
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved = self.save_to_database(all_topics)
            
            duration = time.time() - start_time
            logger.info("\n" + "="*50)
            logger.info("çˆ¬è™«æ‰§è¡Œå®Œæˆ")
            logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y/%m/%d %H:%M:%S')}")
            logger.info(f"âŒ› æ€»è€—æ—¶: {duration:.2f}s")
            logger.info(f"ğŸ“Š å…±çˆ¬å–: {len(all_topics)} æ¡è¯é¢˜")
            logger.info(f"ğŸ’¾ å·²ä¿å­˜: {saved} æ¡è¯é¢˜")
            logger.info("="*50 + "\n")
            
            return True
        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
            return False
        finally:
            self.close_db()


def main():
    """ä¸»å‡½æ•°"""
    spider = HotTopicsSpider()
    spider.run()


if __name__ == '__main__':
    main()
